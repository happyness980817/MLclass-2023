# Linear Regression practice

# -*- coding: utf-8 -*-
"""Assignment4_2019450033_code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xnHxTsX6bvZN2GZs2TD2Nu54scrGBD9B

1.TV 를 입력, Sales 를 출력으로 하는 Linear regression model 구현
  Batch Gradient Descent 방식으로 학습
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

data = pd.read_csv("/content/advertising.csv")
data

X = data['TV']
Y = data['Sales']

type(X)

# Batch Gradient Descent

list_x = []
for x in X:
  list_x.append(x)

list_y = []
for y in Y:
  list_y.append(y)

parameter_w1 = 0.1
parameter_w2 = 1

num_epochs = 1000 # 모델을 수렴하게 하는 Iteration 수

learning_R = 0.00001 # Learning rate


def deriv_J1(X,Y,parameter_w1,parameter_w2):
  sum_error = 0
  for i in range(len(list_x)):
    sum_error += X[i]*(parameter_w1*X[i] + parameter_w2 - Y[i])
  return sum_error

def deriv_J2(X,Y,parameter_w1,parameter_w2):
  sum_error = 0
  for i in range(len(list_x)):
    sum_error += (parameter_w1*X[i] + parameter_w2 - Y[i])
  return sum_error

for i in range(num_epochs):
  gradient_w1 = deriv_J1(X,Y,parameter_w1,parameter_w2)
  gradient_w2 = deriv_J2(X,Y,parameter_w1,parameter_w2)

  parameter_w1 = parameter_w1 - learning_R*gradient_w1/num_epochs
  parameter_w2 = parameter_w2 - learning_R*gradient_w2/num_epochs

y_pred = []
for i in range(len(list_x)):
  y_pred.append([parameter_w1*X[i] + parameter_w2])

parameter_w1,parameter_w2

plt.scatter(list_x,list_y)
plt.xlabel('TV')
plt.ylabel('Sales')
plt.grid(True)
plt.show()

plt.scatter(list_x,list_y)
plt.xlabel('TV')
plt.ylabel('Sales')
plt.plot(list_x,y_pred,'g-' , label='Batch Gradient Descent')
plt.grid(True)
plt.legend()
plt.show()

"""
선택과제 1. Stochastic Gradient Descent 이용하여 epoch 수와 수렴한 모델을 도시
"""

# Stochastic Gradient Descent

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

data = pd.read_csv("/content/advertising.csv")
data

X = data['TV']
Y = data['Sales']

list_x = []
for x in X:
    list_x.append(x)

list_y = []
for y in Y:
    list_y.append(y)

parameter_w1_stoch = 1
parameter_w2_stoch = 1

def deriv_J1(x,y):
    f = x*(parameter_w1_stoch*x + parameter_w2_stoch - y)
    return f

def deriv_J2(x,y):
    f = (parameter_w1_stoch*x + parameter_w2_stoch - y)
    return f

learning_R = 0.00001

epochs = 50

for epoch in range(epochs):
    for i in range(len(list_x)):
        parameter_w1_stoch = parameter_w1_stoch - learning_R * deriv_J1(list_x[i], list_y[i])
        parameter_w2_stoch = parameter_w2_stoch - learning_R * deriv_J2(list_x[i], list_y[i])

y_pred_stoch = []

for x in list_x:
    y_pred_stoch.append(parameter_w1_stoch*x + parameter_w2_stoch)

parameter_w1_stoch,parameter_w2_stoch

plt.scatter(list_x, list_y)
plt.xlabel('TV')
plt.ylabel('Sales')
plt.plot(list_x, y_pred_stoch, 'r-',label='Stochastic Gradient Descent')
plt.grid(True)
plt.legend()
plt.show()

plt.scatter(list_x, list_y)
plt.xlabel('TV')
plt.ylabel('Sales')
plt.plot(list_x, y_pred_stoch, 'r-',label='Stochastic Gradient Descent')
plt.plot(list_x,y_pred,'g-',label='Batch Gradient Descent')
plt.grid(True)
plt.legend()
plt.show()

# 선택과제 2: Loss Surface 도시

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

X = list_x
Y = list_y

w1_values = list(np.linspace(-0.2, 0.2, 100))
w2_values = list(np.linspace(-20, 20, 100))

def J_w(parameter_w1, parameter_w2, X, Y):
    error = 0
    for x, y in zip(X, Y):
        error += (parameter_w1 * x + parameter_w2 - y) ** 2
    return 0.5 * error

W1 = []
W2 = []
Z = []

for w1 in w1_values:
    for w2 in w2_values:
        W1.append(w1)
        W2.append(w2)
        Z.append(J_w(w1, w2, X, Y))

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

ax.plot_trisurf(W1, W2, Z, cmap='viridis')
ax.set_title('Error Surface')
ax.set_xlabel('w1')
ax.set_ylabel('w2')
ax.set_zlabel('Error')

plt.show()